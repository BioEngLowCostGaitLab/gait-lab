{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joear\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(arr,grayscale):\n",
    "    \"\"\" Pads an image if taken near the edge \"\"\"\n",
    "    if grayscale:\n",
    "        arr = np.reshape(arr,(arr.shape[0],arr.shape[1],1))\n",
    "        r = np.zeros((24,24,1))\n",
    "        r[:arr.shape[0],:arr.shape[1],:arr.shape[2]] = arr\n",
    "    else:\n",
    "        r = np.zeros((24,24,3))\n",
    "        r[:arr.shape[0],:arr.shape[1],:arr.shape[2]] = arr\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_np(x_np, y_np):\n",
    "    \"\"\" randomises a numpy array \"\"\"\n",
    "    prng = RandomState(0)\n",
    "    randomise = prng.permutation(x_np.shape[0])\n",
    "    return x_np[randomise], y_np[randomise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_np(x_data, y_data, percent):\n",
    "    \"\"\" splits a numpy array into testing and training \"\"\"\n",
    "    position = int(len(x_data) * (1-percent))\n",
    "    x_train, x_test = x_data[:position], x_data[position:]\n",
    "    y_train, y_test = y_data[:position], y_data[position:]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(randomise=True,grayscale=True):\n",
    "    \"\"\" Loads in image data as numpy arrays \"\"\"\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    none_count = 0 \n",
    "    filedir = join(os.getcwd(),\"..\",\"labels\")\n",
    "    for file in os.listdir(filedir):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            if grayscale:\n",
    "                img = cv.imread(join(filedir,file), cv.IMREAD_GRAYSCALE)\n",
    "            else: \n",
    "                img = cv.imread(join(filedir,file), cv.IMREAD_UNCHANGED)\n",
    "            if not img is None:\n",
    "                img = pad(img,grayscale)\n",
    "                x_values.append(img)\n",
    "                if int(file[0]) == 0:    \n",
    "                    y_values.append([0,1])\n",
    "                else:\n",
    "                    y_values.append([1,0])    \n",
    "            else:\n",
    "                none_count += 1\n",
    "    if none_count > 0:\n",
    "        print(\"None count: \", none_count)\n",
    "    shape = list(x_values[0].shape)\n",
    "    shape[:0] = [len(x_values)]\n",
    "    x_np = np.concatenate(x_values).reshape(shape)\n",
    "    y_np = np.array(y_values)\n",
    "    x_np, y_np = random_np(x_np, y_np)\n",
    "    return x_np, y_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_x(x_train, x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (2251, 24, 24, 3)\n",
      "x_test shape: (563, 24, 24, 3)\n",
      "y_train shape: (2251, 2)\n",
      "y_test shape: (563, 2)\n",
      "Training samples: 2251\n",
      "Test samples: 563\n"
     ]
    }
   ],
   "source": [
    "x_np, y_np = load_data(grayscale=False)\n",
    "x_train, y_train, x_test, y_test = split_np(x_np, y_np,0.2)\n",
    "x_train, x_test = norm_x(x_train, x_test)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(\"Training samples: {}\".format(x_train.shape[0]))\n",
    "print(\"Test samples: {}\".format(x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=25, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"dnn/tmp_best_weights.hdf5\", verbose=0, save_best_only=True) # save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2251 samples, validate on 563 samples\n",
      "Epoch 1/1000\n",
      " - 5s - loss: 0.4534 - acc: 0.7526 - val_loss: 0.1614 - val_acc: 0.9432\n",
      "Epoch 2/1000\n",
      " - 4s - loss: 0.2493 - acc: 0.8920 - val_loss: 0.1164 - val_acc: 0.9449\n",
      "Epoch 3/1000\n",
      " - 4s - loss: 0.1426 - acc: 0.9343 - val_loss: 0.0924 - val_acc: 0.9520\n",
      "Epoch 4/1000\n",
      " - 4s - loss: 0.1136 - acc: 0.9494 - val_loss: 0.0875 - val_acc: 0.9663\n",
      "Epoch 5/1000\n",
      " - 5s - loss: 0.1029 - acc: 0.9565 - val_loss: 0.0882 - val_acc: 0.9680\n",
      "Epoch 6/1000\n",
      " - 4s - loss: 0.1196 - acc: 0.9636 - val_loss: 0.0594 - val_acc: 0.9840\n",
      "Epoch 7/1000\n",
      " - 5s - loss: 0.0829 - acc: 0.9756 - val_loss: 0.0522 - val_acc: 0.9893\n",
      "Epoch 8/1000\n",
      " - 5s - loss: 0.0593 - acc: 0.9862 - val_loss: 0.0454 - val_acc: 0.9840\n",
      "Epoch 9/1000\n",
      " - 7s - loss: 0.0690 - acc: 0.9800 - val_loss: 0.0446 - val_acc: 0.9840\n",
      "Epoch 10/1000\n",
      " - 6s - loss: 0.0491 - acc: 0.9853 - val_loss: 0.0401 - val_acc: 0.9893\n",
      "Epoch 11/1000\n",
      " - 5s - loss: 0.0427 - acc: 0.9862 - val_loss: 0.0364 - val_acc: 0.9876\n",
      "Epoch 12/1000\n",
      " - 4s - loss: 0.0453 - acc: 0.9853 - val_loss: 0.0412 - val_acc: 0.9876\n",
      "Epoch 13/1000\n",
      " - 5s - loss: 0.0451 - acc: 0.9884 - val_loss: 0.0339 - val_acc: 0.9893\n",
      "Epoch 14/1000\n",
      " - 6s - loss: 0.0305 - acc: 0.9907 - val_loss: 0.0396 - val_acc: 0.9893\n",
      "Epoch 15/1000\n",
      " - 5s - loss: 0.0421 - acc: 0.9876 - val_loss: 0.0322 - val_acc: 0.9893\n",
      "Epoch 16/1000\n",
      " - 5s - loss: 0.0256 - acc: 0.9924 - val_loss: 0.0313 - val_acc: 0.9911\n",
      "Epoch 17/1000\n",
      " - 5s - loss: 0.0354 - acc: 0.9889 - val_loss: 0.0278 - val_acc: 0.9929\n",
      "Epoch 18/1000\n",
      " - 5s - loss: 0.0247 - acc: 0.9933 - val_loss: 0.0372 - val_acc: 0.9893\n",
      "Epoch 19/1000\n",
      " - 5s - loss: 0.0188 - acc: 0.9956 - val_loss: 0.0303 - val_acc: 0.9911\n",
      "Epoch 20/1000\n",
      " - 4s - loss: 0.0149 - acc: 0.9951 - val_loss: 0.0312 - val_acc: 0.9911\n",
      "Epoch 21/1000\n",
      " - 5s - loss: 0.0314 - acc: 0.9911 - val_loss: 0.0318 - val_acc: 0.9911\n",
      "Epoch 22/1000\n",
      " - 6s - loss: 0.0233 - acc: 0.9951 - val_loss: 0.0242 - val_acc: 0.9947\n",
      "Epoch 23/1000\n",
      " - 6s - loss: 0.0185 - acc: 0.9942 - val_loss: 0.0226 - val_acc: 0.9947\n",
      "Epoch 24/1000\n",
      " - 5s - loss: 0.0155 - acc: 0.9956 - val_loss: 0.0237 - val_acc: 0.9947\n",
      "Epoch 25/1000\n",
      " - 5s - loss: 0.0134 - acc: 0.9964 - val_loss: 0.0240 - val_acc: 0.9929\n",
      "Epoch 26/1000\n",
      " - 5s - loss: 0.0089 - acc: 0.9987 - val_loss: 0.0257 - val_acc: 0.9911\n",
      "Epoch 27/1000\n",
      " - 7s - loss: 0.0100 - acc: 0.9964 - val_loss: 0.0282 - val_acc: 0.9929\n",
      "Epoch 28/1000\n",
      " - 7s - loss: 0.0119 - acc: 0.9947 - val_loss: 0.0238 - val_acc: 0.9947\n",
      "Epoch 29/1000\n",
      " - 6s - loss: 0.0081 - acc: 0.9987 - val_loss: 0.0364 - val_acc: 0.9929\n",
      "Epoch 30/1000\n",
      " - 6s - loss: 0.0095 - acc: 0.9973 - val_loss: 0.0310 - val_acc: 0.9929\n",
      "Epoch 31/1000\n",
      " - 5s - loss: 0.0086 - acc: 0.9978 - val_loss: 0.0252 - val_acc: 0.9929\n",
      "Epoch 32/1000\n",
      " - 6s - loss: 0.0070 - acc: 0.9973 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 33/1000\n",
      " - 5s - loss: 0.0112 - acc: 0.9973 - val_loss: 0.0248 - val_acc: 0.9929\n",
      "Epoch 34/1000\n",
      " - 5s - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0385 - val_acc: 0.9929\n",
      "Epoch 35/1000\n",
      " - 6s - loss: 0.0043 - acc: 0.9982 - val_loss: 0.0357 - val_acc: 0.9929\n",
      "Epoch 36/1000\n",
      " - 5s - loss: 0.0087 - acc: 0.9973 - val_loss: 0.0324 - val_acc: 0.9929\n",
      "Epoch 37/1000\n",
      " - 5s - loss: 0.0068 - acc: 0.9982 - val_loss: 0.0231 - val_acc: 0.9947\n",
      "Epoch 38/1000\n",
      " - 5s - loss: 0.0065 - acc: 0.9987 - val_loss: 0.0174 - val_acc: 0.9947\n",
      "Epoch 39/1000\n",
      " - 6s - loss: 0.0036 - acc: 0.9987 - val_loss: 0.0202 - val_acc: 0.9947\n",
      "Epoch 40/1000\n",
      " - 6s - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0236 - val_acc: 0.9929\n",
      "Epoch 41/1000\n",
      " - 6s - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0379 - val_acc: 0.9947\n",
      "Epoch 42/1000\n",
      " - 4s - loss: 0.0070 - acc: 0.9982 - val_loss: 0.0191 - val_acc: 0.9964\n",
      "Epoch 43/1000\n",
      " - 4s - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0217 - val_acc: 0.9964\n",
      "Epoch 44/1000\n",
      " - 4s - loss: 0.0020 - acc: 0.9996 - val_loss: 0.0244 - val_acc: 0.9947\n",
      "Epoch 45/1000\n",
      " - 5s - loss: 0.0023 - acc: 0.9987 - val_loss: 0.0252 - val_acc: 0.9964\n",
      "Epoch 46/1000\n",
      " - 4s - loss: 0.0062 - acc: 0.9978 - val_loss: 0.0287 - val_acc: 0.9964\n",
      "Epoch 47/1000\n",
      " - 5s - loss: 0.0046 - acc: 0.9982 - val_loss: 0.0338 - val_acc: 0.9947\n",
      "Epoch 48/1000\n",
      " - 5s - loss: 0.0022 - acc: 0.9996 - val_loss: 0.0260 - val_acc: 0.9964\n",
      "Epoch 49/1000\n",
      " - 5s - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0239 - val_acc: 0.9964\n",
      "Epoch 50/1000\n",
      " - 4s - loss: 0.0027 - acc: 0.9991 - val_loss: 0.0367 - val_acc: 0.9929\n",
      "Epoch 51/1000\n",
      " - 4s - loss: 0.0013 - acc: 0.9996 - val_loss: 0.0237 - val_acc: 0.9947\n",
      "Epoch 52/1000\n",
      " - 5s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0247 - val_acc: 0.9964\n",
      "Epoch 53/1000\n",
      " - 5s - loss: 0.0065 - acc: 0.9987 - val_loss: 0.0197 - val_acc: 0.9964\n",
      "Epoch 54/1000\n",
      " - 5s - loss: 0.0026 - acc: 0.9991 - val_loss: 0.0327 - val_acc: 0.9929\n",
      "Epoch 55/1000\n",
      " - 5s - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0243 - val_acc: 0.9964\n",
      "Epoch 56/1000\n",
      " - 4s - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0266 - val_acc: 0.9964\n",
      "Epoch 57/1000\n",
      " - 5s - loss: 7.9585e-04 - acc: 1.0000 - val_loss: 0.0287 - val_acc: 0.9964\n",
      "Epoch 58/1000\n",
      " - 4s - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0201 - val_acc: 0.9964\n",
      "Epoch 59/1000\n",
      " - 4s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0265 - val_acc: 0.9964\n",
      "Epoch 60/1000\n",
      " - 4s - loss: 0.0047 - acc: 0.9982 - val_loss: 0.0230 - val_acc: 0.9964\n",
      "Epoch 61/1000\n",
      " - 5s - loss: 7.2518e-04 - acc: 1.0000 - val_loss: 0.0270 - val_acc: 0.9964\n",
      "Epoch 62/1000\n",
      " - 5s - loss: 8.0229e-04 - acc: 1.0000 - val_loss: 0.0240 - val_acc: 0.9964\n",
      "Epoch 63/1000\n",
      " - 4s - loss: 4.4295e-04 - acc: 1.0000 - val_loss: 0.0283 - val_acc: 0.9964\n",
      "Epoch 00063: early stopping\n",
      "Test loss: 0.0174371857736956\n",
      "Test accuracy: 0.9946714031971581\n",
      "Elapsed time: 0:05:20.05\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[monitor,checkpointer])\n",
    "model.load_weights('dnn/tmp_best_weights.hdf5') # load weights from best model\n",
    "\n",
    "\n",
    "save_dir = join(os.getcwd(),\"dnn\")\n",
    "save_path = join(save_dir,str(int(start_time)) + \"_cnn.h5\")\n",
    "model.save(save_path)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "save_dir = join(os.getcwd(),\"dnn\")\n",
    "save_path = join(save_dir,\"1528151909_cnn.h5\")\n",
    "\n",
    "model2 = load_model(save_path)\n",
    "pred = model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0\n",
      " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1\n",
      " 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
      " 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0\n",
      " 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0\n",
      " 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1\n",
      " 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
      " 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 1 1 0 0 0]\n",
      "Expected: [1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0\n",
      " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1\n",
      " 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
      " 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0\n",
      " 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0\n",
      " 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1\n",
      " 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
      " 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "print(\"Predictions: {}\".format(predict_classes))\n",
    "print(\"Expected: {}\".format(expected_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9946714031971581\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(\"Accuracy: {}\".format(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
