{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joear\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(arr,grayscale):\n",
    "    \"\"\" Pads an image if taken near the edge \"\"\"\n",
    "    if grayscale:\n",
    "        arr = np.reshape(arr,(arr.shape[0],arr.shape[1],1))\n",
    "        r = np.zeros((24,24,1))\n",
    "        r[:arr.shape[0],:arr.shape[1],:arr.shape[2]] = arr\n",
    "    else:\n",
    "        r = np.zeros((24,24,3))\n",
    "        r[:arr.shape[0],:arr.shape[1],:arr.shape[2]] = arr\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_np(x_np, y_np):\n",
    "    \"\"\" randomises a numpy array \"\"\"\n",
    "    prng = RandomState(0)\n",
    "    randomise = prng.permutation(x_np.shape[0])\n",
    "    return x_np[randomise], y_np[randomise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_np(x_data, y_data, percent):\n",
    "    \"\"\" splits a numpy array into testing and training \"\"\"\n",
    "    position = int(len(x_data) * (1-percent))\n",
    "    x_train, x_test = x_data[:position], x_data[position:]\n",
    "    y_train, y_test = y_data[:position], y_data[position:]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(randomise=True,grayscale=True):\n",
    "    \"\"\" Loads in image data as numpy arrays \"\"\"\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    none_count = 0 \n",
    "    filedir = join(os.getcwd(),\"..\",\"labels\")\n",
    "    for file in os.listdir(filedir):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            if grayscale:\n",
    "                img = cv.imread(join(filedir,file), cv.IMREAD_GRAYSCALE)\n",
    "            else: \n",
    "                img = cv.imread(join(filedir,file), cv.IMREAD_UNCHANGED)\n",
    "            if not img is None:\n",
    "                img = pad(img,grayscale)\n",
    "                x_values.append(img)\n",
    "                if int(file[0]) == 0:    \n",
    "                    y_values.append([0,1])\n",
    "                else:\n",
    "                    y_values.append([1,0])    \n",
    "            else:\n",
    "                none_count += 1\n",
    "    if none_count > 0:\n",
    "        print(\"None count: \", none_count)\n",
    "    shape = list(x_values[0].shape)\n",
    "    shape[:0] = [len(x_values)]\n",
    "    x_np = np.concatenate(x_values).reshape(shape)\n",
    "    y_np = np.array(y_values)\n",
    "    x_np, y_np = random_np(x_np, y_np)\n",
    "    return x_np, y_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_x(x_train, x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (2251, 24, 24, 1)\n",
      "x_test shape: (563, 24, 24, 1)\n",
      "y_train shape: (2251, 2)\n",
      "y_test shape: (563, 2)\n",
      "Training samples: 2251\n",
      "Test samples: 563\n"
     ]
    }
   ],
   "source": [
    "x_np, y_np = load_data(True)\n",
    "x_train, y_train, x_test, y_test = split_np(x_np, y_np,0.2)\n",
    "x_train, x_test = norm_x(x_train, x_test)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(\"Training samples: {}\".format(x_train.shape[0]))\n",
    "print(\"Test samples: {}\".format(x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=25, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"dnn/tmp_best_weights.hdf5\", verbose=0, save_best_only=True) # save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2251 samples, validate on 563 samples\n",
      "Epoch 1/1000\n",
      " - 6s - loss: 0.4676 - acc: 0.7570 - val_loss: 0.4419 - val_acc: 0.8082\n",
      "Epoch 2/1000\n",
      " - 5s - loss: 0.2543 - acc: 0.9027 - val_loss: 0.1496 - val_acc: 0.9378\n",
      "Epoch 3/1000\n",
      " - 5s - loss: 0.1845 - acc: 0.9156 - val_loss: 0.1341 - val_acc: 0.9432\n",
      "Epoch 4/1000\n",
      " - 5s - loss: 0.1657 - acc: 0.9325 - val_loss: 0.1182 - val_acc: 0.9449\n",
      "Epoch 5/1000\n",
      " - 7s - loss: 0.1462 - acc: 0.9360 - val_loss: 0.1262 - val_acc: 0.9627\n",
      "Epoch 6/1000\n",
      " - 6s - loss: 0.1237 - acc: 0.9462 - val_loss: 0.1031 - val_acc: 0.9645\n",
      "Epoch 7/1000\n",
      " - 7s - loss: 0.1357 - acc: 0.9507 - val_loss: 0.1084 - val_acc: 0.9680\n",
      "Epoch 8/1000\n",
      " - 7s - loss: 0.1109 - acc: 0.9569 - val_loss: 0.1121 - val_acc: 0.9609\n",
      "Epoch 9/1000\n",
      " - 5s - loss: 0.0978 - acc: 0.9605 - val_loss: 0.0914 - val_acc: 0.9734\n",
      "Epoch 10/1000\n",
      " - 5s - loss: 0.0984 - acc: 0.9596 - val_loss: 0.0924 - val_acc: 0.9680\n",
      "Epoch 11/1000\n",
      " - 5s - loss: 0.0951 - acc: 0.9671 - val_loss: 0.0778 - val_acc: 0.9787\n",
      "Epoch 12/1000\n",
      " - 5s - loss: 0.0922 - acc: 0.9636 - val_loss: 0.0773 - val_acc: 0.9822\n",
      "Epoch 13/1000\n",
      " - 6s - loss: 0.0795 - acc: 0.9756 - val_loss: 0.0713 - val_acc: 0.9805\n",
      "Epoch 14/1000\n",
      " - 5s - loss: 0.0913 - acc: 0.9711 - val_loss: 0.0683 - val_acc: 0.9840\n",
      "Epoch 15/1000\n",
      " - 5s - loss: 0.0663 - acc: 0.9787 - val_loss: 0.0679 - val_acc: 0.9840\n",
      "Epoch 16/1000\n",
      " - 5s - loss: 0.0661 - acc: 0.9729 - val_loss: 0.0642 - val_acc: 0.9840\n",
      "Epoch 17/1000\n",
      " - 5s - loss: 0.0674 - acc: 0.9796 - val_loss: 0.0588 - val_acc: 0.9840\n",
      "Epoch 18/1000\n",
      " - 7s - loss: 0.0701 - acc: 0.9765 - val_loss: 0.0644 - val_acc: 0.9822\n",
      "Epoch 19/1000\n",
      " - 7s - loss: 0.0614 - acc: 0.9791 - val_loss: 0.0565 - val_acc: 0.9805\n",
      "Epoch 20/1000\n",
      " - 7s - loss: 0.0566 - acc: 0.9800 - val_loss: 0.0537 - val_acc: 0.9858\n",
      "Epoch 21/1000\n",
      " - 8s - loss: 0.0574 - acc: 0.9827 - val_loss: 0.0485 - val_acc: 0.9822\n",
      "Epoch 22/1000\n",
      " - 5s - loss: 0.0694 - acc: 0.9805 - val_loss: 0.0542 - val_acc: 0.9858\n",
      "Epoch 23/1000\n",
      " - 5s - loss: 0.0525 - acc: 0.9876 - val_loss: 0.0489 - val_acc: 0.9876\n",
      "Epoch 24/1000\n",
      " - 5s - loss: 0.0419 - acc: 0.9876 - val_loss: 0.0463 - val_acc: 0.9858\n",
      "Epoch 25/1000\n",
      " - 6s - loss: 0.0575 - acc: 0.9818 - val_loss: 0.0528 - val_acc: 0.9840\n",
      "Epoch 26/1000\n",
      " - 9s - loss: 0.0472 - acc: 0.9827 - val_loss: 0.0485 - val_acc: 0.9858\n",
      "Epoch 27/1000\n",
      " - 9s - loss: 0.0354 - acc: 0.9893 - val_loss: 0.0446 - val_acc: 0.9876\n",
      "Epoch 28/1000\n",
      " - 7s - loss: 0.0404 - acc: 0.9867 - val_loss: 0.0490 - val_acc: 0.9893\n",
      "Epoch 29/1000\n",
      " - 7s - loss: 0.0351 - acc: 0.9880 - val_loss: 0.0434 - val_acc: 0.9893\n",
      "Epoch 30/1000\n",
      " - 6s - loss: 0.0394 - acc: 0.9880 - val_loss: 0.0432 - val_acc: 0.9858\n",
      "Epoch 31/1000\n",
      " - 8s - loss: 0.0347 - acc: 0.9898 - val_loss: 0.0428 - val_acc: 0.9876\n",
      "Epoch 32/1000\n",
      " - 9s - loss: 0.0357 - acc: 0.9907 - val_loss: 0.0431 - val_acc: 0.9893\n",
      "Epoch 33/1000\n",
      " - 6s - loss: 0.0373 - acc: 0.9862 - val_loss: 0.0518 - val_acc: 0.9858\n",
      "Epoch 34/1000\n",
      " - 6s - loss: 0.0373 - acc: 0.9893 - val_loss: 0.0421 - val_acc: 0.9876\n",
      "Epoch 35/1000\n",
      " - 6s - loss: 0.0312 - acc: 0.9893 - val_loss: 0.0355 - val_acc: 0.9929\n",
      "Epoch 36/1000\n",
      " - 5s - loss: 0.0213 - acc: 0.9929 - val_loss: 0.0358 - val_acc: 0.9929\n",
      "Epoch 37/1000\n",
      " - 6s - loss: 0.0284 - acc: 0.9920 - val_loss: 0.0372 - val_acc: 0.9893\n",
      "Epoch 38/1000\n",
      " - 7s - loss: 0.0339 - acc: 0.9880 - val_loss: 0.0423 - val_acc: 0.9876\n",
      "Epoch 39/1000\n",
      " - 5s - loss: 0.0223 - acc: 0.9938 - val_loss: 0.0365 - val_acc: 0.9893\n",
      "Epoch 40/1000\n",
      " - 5s - loss: 0.0225 - acc: 0.9942 - val_loss: 0.0440 - val_acc: 0.9893\n",
      "Epoch 41/1000\n",
      " - 6s - loss: 0.0219 - acc: 0.9929 - val_loss: 0.0427 - val_acc: 0.9911\n",
      "Epoch 42/1000\n",
      " - 6s - loss: 0.0197 - acc: 0.9938 - val_loss: 0.0522 - val_acc: 0.9893\n",
      "Epoch 43/1000\n",
      " - 5s - loss: 0.0254 - acc: 0.9942 - val_loss: 0.0369 - val_acc: 0.9911\n",
      "Epoch 44/1000\n",
      " - 4s - loss: 0.0152 - acc: 0.9951 - val_loss: 0.0381 - val_acc: 0.9893\n",
      "Epoch 45/1000\n",
      " - 5s - loss: 0.0284 - acc: 0.9938 - val_loss: 0.0401 - val_acc: 0.9876\n",
      "Epoch 46/1000\n",
      " - 4s - loss: 0.0176 - acc: 0.9956 - val_loss: 0.0424 - val_acc: 0.9893\n",
      "Epoch 47/1000\n",
      " - 4s - loss: 0.0202 - acc: 0.9933 - val_loss: 0.0425 - val_acc: 0.9911\n",
      "Epoch 48/1000\n",
      " - 4s - loss: 0.0276 - acc: 0.9942 - val_loss: 0.0432 - val_acc: 0.9876\n",
      "Epoch 49/1000\n",
      " - 4s - loss: 0.0148 - acc: 0.9951 - val_loss: 0.0347 - val_acc: 0.9876\n",
      "Epoch 50/1000\n",
      " - 4s - loss: 0.0121 - acc: 0.9964 - val_loss: 0.0415 - val_acc: 0.9893\n",
      "Epoch 51/1000\n",
      " - 4s - loss: 0.0151 - acc: 0.9956 - val_loss: 0.0350 - val_acc: 0.9893\n",
      "Epoch 52/1000\n",
      " - 5s - loss: 0.0086 - acc: 0.9978 - val_loss: 0.0534 - val_acc: 0.9876\n",
      "Epoch 53/1000\n",
      " - 4s - loss: 0.0154 - acc: 0.9964 - val_loss: 0.0497 - val_acc: 0.9876\n",
      "Epoch 54/1000\n",
      " - 4s - loss: 0.0112 - acc: 0.9960 - val_loss: 0.0348 - val_acc: 0.9893\n",
      "Epoch 55/1000\n",
      " - 4s - loss: 0.0089 - acc: 0.9987 - val_loss: 0.0481 - val_acc: 0.9876\n",
      "Epoch 00055: early stopping\n",
      "Test loss: 0.03470412097403161\n",
      "Test accuracy: 0.9875666074600356\n",
      "Elapsed time: 0:05:18.02\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[monitor,checkpointer])\n",
    "model.load_weights('dnn/tmp_best_weights.hdf5') # load weights from best model\n",
    "\n",
    "\n",
    "save_dir = join(os.getcwd(),\"dnn\")\n",
    "save_path = join(save_dir,str(int(start_time)) + \"_cnn.h5\")\n",
    "model.save(save_path)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "save_dir = join(os.getcwd(),\"dnn\")\n",
    "save_path = join(save_dir,\"1528149749_cnn.h5\")\n",
    "\n",
    "model2 = load_model(save_path)\n",
    "pred = model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0\n",
      " 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1\n",
      " 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1\n",
      " 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 0 0 0 1 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0\n",
      " 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 0\n",
      " 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1\n",
      " 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
      " 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 1 1 0 0 0]\n",
      "Expected: [1 1 0 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0\n",
      " 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1\n",
      " 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1\n",
      " 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 1 1 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1\n",
      " 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1\n",
      " 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 1 1 1 0 1 1 1 0\n",
      " 0 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0\n",
      " 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 1\n",
      " 0 1 0 0 1 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 0 0 1 0 0 0 0 1 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 1 0\n",
      " 1 0 1 0 1 1 1 0 0 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 0 0 0 0\n",
      " 1 0 1 0 0 1 0 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 0 1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "print(\"Predictions: {}\".format(predict_classes))\n",
    "print(\"Expected: {}\".format(expected_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9875666074600356\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(\"Accuracy: {}\".format(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
