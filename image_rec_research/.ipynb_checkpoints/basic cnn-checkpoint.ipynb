{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joear\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(arr,grayscale):\n",
    "    \"\"\" Pads an image if taken near the edge \"\"\"\n",
    "    if grayscale:\n",
    "        arr = np.reshape(arr,(arr.shape[0],arr.shape[1],1))\n",
    "        r = np.zeros((50,50,1))\n",
    "        r[:arr.shape[0],:arr.shape[1],:arr.shape[2]] = arr\n",
    "    else:\n",
    "        r = np.zeros((50,50,3))\n",
    "        r[:arr.shape[0],:arr.shape[1],:arr.shape[2]] = arr\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_np(x_np, y_np):\n",
    "    \"\"\" randomises a numpy array \"\"\"\n",
    "    prng = RandomState(0)\n",
    "    randomise = prng.permutation(x_np.shape[0])\n",
    "    return x_np[randomise], y_np[randomise]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_np(x_data, y_data, percent):\n",
    "    \"\"\" splits a numpy array into testing and training \"\"\"\n",
    "    position = int(len(x_data) * (1-percent))\n",
    "    x_train, x_test = x_data[:position], x_data[position:]\n",
    "    y_train, y_test = y_data[:position], y_data[position:]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(randomise=True,grayscale=True):\n",
    "    \"\"\" Loads in image data as numpy arrays \"\"\"\n",
    "    x_values = []\n",
    "    y_values = []\n",
    "    none_count = 0 \n",
    "    filedir = join(os.getcwd(),\"labels\")\n",
    "    for file in os.listdir(filedir):\n",
    "        if file.endswith(\".jpg\"):\n",
    "            if grayscale:\n",
    "                img = cv.imread(join(filedir,file), cv.IMREAD_GRAYSCALE)\n",
    "            else: \n",
    "                img = cv.imread(join(filedir,file), cv.IMREAD_UNCHANGED)\n",
    "            if not img is None:\n",
    "                img = pad(img,grayscale)\n",
    "                x_values.append(img)\n",
    "                ball_type = int(file.split(\"_\")[2])\n",
    "                \n",
    "                if ball_type == 0:    \n",
    "                    y_values.append([0,1])\n",
    "                else:\n",
    "                    y_values.append([1,0])    \n",
    "            else:\n",
    "                none_count += 1\n",
    "    if none_count > 0:\n",
    "        print(\"None count: \", none_count)\n",
    "    shape = list(x_values[0].shape)\n",
    "    shape[:0] = [len(x_values)]\n",
    "    x_np = np.concatenate(x_values).reshape(shape)\n",
    "    y_np = np.array(y_values)\n",
    "    x_np, y_np = random_np(x_np, y_np)\n",
    "    return x_np, y_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_x(x_train, x_test):\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None count:  43\n",
      "x_train shape: (1292, 50, 50, 3)\n",
      "x_test shape: (323, 50, 50, 3)\n",
      "y_train shape: (1292, 2)\n",
      "y_test shape: (323, 2)\n",
      "Training samples: 1292\n",
      "Test samples: 323\n"
     ]
    }
   ],
   "source": [
    "x_np, y_np = load_data(grayscale=False)\n",
    "x_train, y_train, x_test, y_test = split_np(x_np, y_np,0.2)\n",
    "x_train, x_test = norm_x(x_train, x_test)\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(\"Training samples: {}\".format(x_train.shape[0]))\n",
    "print(\"Test samples: {}\".format(x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=25, verbose=1, mode='auto')\n",
    "checkpointer = ModelCheckpoint(filepath=\"dnn/tmp_best_weights_bin_cnn.hdf5\", verbose=0, save_best_only=True) # save best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1292 samples, validate on 323 samples\n",
      "Epoch 1/1000\n",
      " - 47s - loss: 0.6303 - acc: 0.6238 - val_loss: 0.5495 - val_acc: 0.6471\n",
      "Epoch 2/1000\n",
      " - 34s - loss: 0.5495 - acc: 0.6711 - val_loss: 0.3451 - val_acc: 0.6471\n",
      "Epoch 3/1000\n",
      " - 24s - loss: 0.3226 - acc: 0.8181 - val_loss: 0.1034 - val_acc: 0.9907\n",
      "Epoch 4/1000\n",
      " - 34s - loss: 0.1248 - acc: 0.9512 - val_loss: 0.0049 - val_acc: 1.0000\n",
      "Epoch 5/1000\n",
      " - 25s - loss: 0.4006 - acc: 0.9373 - val_loss: 0.0092 - val_acc: 1.0000\n",
      "Epoch 6/1000\n",
      " - 23s - loss: 0.0512 - acc: 0.9876 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 7/1000\n",
      " - 24s - loss: 0.0328 - acc: 0.9915 - val_loss: 1.2480e-04 - val_acc: 1.0000\n",
      "Epoch 8/1000\n",
      " - 23s - loss: 0.0228 - acc: 0.9946 - val_loss: 3.6446e-05 - val_acc: 1.0000\n",
      "Epoch 9/1000\n",
      " - 23s - loss: 0.0151 - acc: 0.9961 - val_loss: 1.4572e-05 - val_acc: 1.0000\n",
      "Epoch 10/1000\n",
      " - 37s - loss: 0.0197 - acc: 0.9961 - val_loss: 8.7032e-06 - val_acc: 1.0000\n",
      "Epoch 11/1000\n",
      " - 37s - loss: 0.0099 - acc: 0.9977 - val_loss: 4.9889e-06 - val_acc: 1.0000\n",
      "Epoch 12/1000\n",
      " - 43s - loss: 0.0090 - acc: 0.9969 - val_loss: 2.3944e-06 - val_acc: 1.0000\n",
      "Epoch 13/1000\n",
      " - 28s - loss: 0.0130 - acc: 0.9954 - val_loss: 1.2518e-06 - val_acc: 1.0000\n",
      "Epoch 14/1000\n",
      " - 26s - loss: 0.0114 - acc: 0.9938 - val_loss: 9.9728e-07 - val_acc: 1.0000\n",
      "Epoch 15/1000\n",
      " - 27s - loss: 0.0094 - acc: 0.9969 - val_loss: 1.3597e-06 - val_acc: 1.0000\n",
      "Epoch 16/1000\n",
      " - 27s - loss: 0.0078 - acc: 0.9969 - val_loss: 4.3920e-07 - val_acc: 1.0000\n",
      "Epoch 17/1000\n",
      " - 24s - loss: 0.0060 - acc: 0.9985 - val_loss: 3.1021e-07 - val_acc: 1.0000\n",
      "Epoch 18/1000\n",
      " - 23s - loss: 0.0029 - acc: 0.9992 - val_loss: 3.3568e-07 - val_acc: 1.0000\n",
      "Epoch 19/1000\n",
      " - 25s - loss: 0.0069 - acc: 0.9969 - val_loss: 3.6373e-07 - val_acc: 1.0000\n",
      "Epoch 20/1000\n",
      " - 22s - loss: 0.0075 - acc: 0.9969 - val_loss: 1.6756e-07 - val_acc: 1.0000\n",
      "Epoch 21/1000\n",
      " - 21s - loss: 0.0029 - acc: 1.0000 - val_loss: 1.8657e-07 - val_acc: 1.0000\n",
      "Epoch 22/1000\n",
      " - 20s - loss: 0.0036 - acc: 1.0000 - val_loss: 1.6055e-07 - val_acc: 1.0000\n",
      "Epoch 23/1000\n",
      " - 20s - loss: 0.0067 - acc: 0.9985 - val_loss: 1.3951e-07 - val_acc: 1.0000\n",
      "Epoch 24/1000\n",
      " - 19s - loss: 0.0021 - acc: 1.0000 - val_loss: 1.2733e-07 - val_acc: 1.0000\n",
      "Epoch 25/1000\n",
      " - 19s - loss: 0.0010 - acc: 1.0000 - val_loss: 1.2272e-07 - val_acc: 1.0000\n",
      "Epoch 26/1000\n",
      " - 18s - loss: 0.0030 - acc: 0.9992 - val_loss: 1.3102e-07 - val_acc: 1.0000\n",
      "Epoch 27/1000\n",
      " - 18s - loss: 0.0015 - acc: 1.0000 - val_loss: 1.6018e-07 - val_acc: 1.0000\n",
      "Epoch 28/1000\n",
      " - 18s - loss: 0.0053 - acc: 0.9977 - val_loss: 1.2179e-07 - val_acc: 1.0000\n",
      "Epoch 29/1000\n",
      " - 17s - loss: 0.0024 - acc: 1.0000 - val_loss: 1.2198e-07 - val_acc: 1.0000\n",
      "Epoch 30/1000\n",
      " - 17s - loss: 7.6248e-04 - acc: 1.0000 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 31/1000\n",
      " - 17s - loss: 0.0027 - acc: 0.9992 - val_loss: 1.1995e-07 - val_acc: 1.0000\n",
      "Epoch 32/1000\n",
      " - 16s - loss: 0.0010 - acc: 1.0000 - val_loss: 1.2032e-07 - val_acc: 1.0000\n",
      "Epoch 33/1000\n",
      " - 17s - loss: 0.0013 - acc: 1.0000 - val_loss: 1.4541e-07 - val_acc: 1.0000\n",
      "Epoch 34/1000\n",
      " - 16s - loss: 0.0018 - acc: 0.9985 - val_loss: 1.2198e-07 - val_acc: 1.0000\n",
      "Epoch 00034: early stopping\n",
      "Test loss: 10.424323725626564\n",
      "Test accuracy: 0.3529411765628555\n",
      "Elapsed time: 0:14:42.16\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 1000\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test),\n",
    "          callbacks=[monitor,checkpointer])\n",
    "model.load_weights('dnn/tmp_best_weights.hdf5') # load weights from best model\n",
    "\n",
    "\n",
    "save_dir = join(os.getcwd(),\"dnn\")\n",
    "save_path = join(save_dir,str(int(start_time)) + \"_cnn.h5\")\n",
    "model.save(save_path)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=2)\n",
    "print('Test loss: {}'.format(score[0]))\n",
    "print('Test accuracy: {}'.format(score[1]))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "save_dir = join(os.getcwd(),\"dnn\")\n",
    "save_path = join(save_dir,\"1528478520_cnn.h5\")\n",
    "\n",
    "model2 = load_model(save_path)\n",
    "pred = model2.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Expected: [0 1 1 1 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1 0\n",
      " 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0\n",
      " 1 1 1 1 0 0 1 0 1 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1\n",
      " 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 0\n",
      " 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1\n",
      " 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "predict_classes = np.argmax(pred,axis=1)\n",
    "expected_classes = np.argmax(y_test,axis=1)\n",
    "print(\"Predictions: {}\".format(predict_classes))\n",
    "print(\"Expected: {}\".format(expected_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35294117647058826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "correct = accuracy_score(expected_classes,predict_classes)\n",
    "print(\"Accuracy: {}\".format(correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
